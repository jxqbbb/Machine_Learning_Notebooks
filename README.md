# Predictive analysis and machine learning model building projects
## 1. Titanic disaster survival prediction
<a href="https://github.com/jxqbbb/Machine_Learning_Notebooks/blob/main/titanic_prediction/titanic_prediction_notebook.ipynb">⏩Notebook⏪</a>
 <html>
  <body>
    The goal of the notebook was to implement the entire data science workflow:
    <ul>
      <li>Loading the dataset (CSV from Kaggle.com)</li>
      <li>Getting to know the data (Understanding the problem, summarizing statistics, exploring using pandas)</li>
      <li>Data cleaning (Filling missing values using different methods, including EDA and machine learning for missing value predictions)</li>
      <li>Exploratory data analysis (Data visualization, statistical testing, correlation analysis)</li>
      <li>Feature selection and engineering (Dropping features, encoding)</li>
      <li>Predictive model selection (Logistic regression, decision tree, random forest, hyperparameter tuning, cross-validation, regularization)</li>
      <li>Evaluation (Submitting predictions to Kaggle.com competition)</li>
    </ul>
  </body>
</html>

## 2. Increased chance of heart attack prediction
<a href="https://github.com/jxqbbb/Machine_Learning_Notebooks/blob/main/heart_attack_prediction/heart_attack_prediction.ipynb">⏩Notebook⏪</a> 

In this notebook, my goal was to maximize the performance of the kNN predictive algorithm. I achieved this by conducting a comprehensive analysis, using different evaluation metrics, and fine-tuning hyperparameters. Additionally, I implemented PCA analysis, pipelines, and various kinds of cross-validation methods to familiarize myself with scikit-learn's implementation of these functionalities.

## 3. The best medicine for the patient - multiclass classification
<a href="https://github.com/jxqbbb/Machine_Learning_Notebooks/blob/main/best_medicine_prediction/best_medicine_prediction.ipynb">⏩Notebook⏪</a> 

This project aims to compare two very different machine learning models: the complex Support Vector Machines (SVM) and the simpler Naive Bayes. During data analysis it turns out that the dataset has a limited number of features and samples. To make things even harder, the features don't seem to provide much information for making accurate predictions and the target classes are highly imbalanced.

The big question is: which model will perform better on this difficult multi-classification problem? By comparing the performance of SVM and Naive Bayes, this project will give us insights into how to choose and use machine learning models in challenging situations.
